{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd36251-0e58-4e3a-8f45-3d917b07033f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931c1c50",
   "metadata": {},
   "source": [
    "# TUTORIAL: Train YOLO model for Rock / Paper / Scissors game\n",
    "\n",
    "![rock-paper-scissors.png](attachment:rock-paper-scissors.png)\n",
    "\n",
    "*A guide to use Transfer Learning on YOLO model in order to play to the Rock / Paper / Scissors game through an AI Notebook.*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to show how it is possible to train YOLO model to play to the game \"rock paper scissors\"!\n",
    "YOLO is an object detection algorithm. Although closely related to image classification, object detection performs image classification on a more precise scale. Object detection locates and categorizes features in images.\n",
    "\n",
    "It is based on the YOLO open source [repository](https://github.com/ultralytics/ultralytics).\n",
    "\n",
    "## Code\n",
    "\n",
    "The different steps are as follow:\n",
    "\n",
    "- Download the Rock / Paper / Scissors Dataset\n",
    "- Install Ultralytics dependencies\n",
    "- Import dependencies and check GPU availability\n",
    "- Retrieve pre-trained YOLO model weights\n",
    "- Test the model on a scissors example\n",
    "- Run YOLO training on ü™® / üìÉ / ‚úÇÔ∏è Dataset\n",
    "- Display results of YOLO training on ü™® / üìÉ / ‚úÇÔ∏è Dataset\n",
    "- Export trained weights for future inference\n",
    "- Create a new YOLO model object based on the exported weights\n",
    "- Test your YOLO custom model on the ü™® / üìÉ / ‚úÇÔ∏è Dataset\n",
    "- Run YOLO inference on new images\n",
    "\n",
    "# Download the Rock / Paper / Scissors Dataset\n",
    "\n",
    "The Rock / Paper / Scissors Dataset is available on <a href=\"https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw\">Roboflow</a>.\n",
    "\n",
    "If you want to use this **Public Dataset** on the tutorial, follow the next requirements:\n",
    "\n",
    "- create a Roboflow account\n",
    "- click on `Download this dataset` in order to download the collection of data\n",
    "- select `YOLO xx` format\n",
    "- choose the method `show download code`\n",
    "- confirm by clicking on the `Continue` button\n",
    "\n",
    "In the window that opens, click on the `>_ Terminal` tab. You will get a `curl` command, followed by your `<dataset_url>`, that will allow you to download your dataset directly inside the notebook.\n",
    "\n",
    "To download it, replace `<dataset_url>` with yours in the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the folder corresponding to your object container\n",
    "%cd /workspace/data\n",
    "\n",
    "# unzip downloaded dataset zip file silently (-q for quiet mode)\n",
    "# make sure to prefix the curl command with a ! to ensure that it is executed as a shell command (instead of Python)\n",
    "!curl -L \"<dataset_url>\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66323553",
   "metadata": {},
   "source": [
    "## Install Ultralytics dependencies\n",
    "The easiest way to use YOLO is to install Python dependency for ultralytics\n",
    "> Feel free to install more recent version than 8.3.3 if there is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics==8.3.3 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606369e1",
   "metadata": {},
   "source": [
    "## Import dependencies and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1778eb7-9a6c-49f2-b834-820300cd5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45871a43",
   "metadata": {},
   "source": [
    "## Retrieve pre-trained YOLO model weights\n",
    "\n",
    "In this tutorial, we will do **Transfer Learning** based on a YOLO model pre-trained on the <a href=\"https://cocodataset.org/\">COCO dataset</a>.\n",
    "\n",
    "**How to define Transfer Learning?**\n",
    "\n",
    "For both humans and machines, learning something new takes time and practice. However, it is easier to perform similar tasks to those already learned. As with humans, AI will be able to identify patterns from previous knowledge and apply them to new learning.\n",
    "\n",
    "If a model is trained on a database, there is no need to re-train the model from scratch to fit a new set of similar data.\n",
    "\n",
    "Main advantages of Transfer Learning:\n",
    "\n",
    "- saving resources\n",
    "- improving efficiency\n",
    "- model training facilitation\n",
    "- saving time\n",
    "\n",
    "**What is the COCO dataset?**\n",
    "\n",
    "COCO is a large-scale object detection, segmentation, and also captioning dataset. COCO has several features:\n",
    "\n",
    "- Object segmentation\n",
    "- Recognition in context\n",
    "- Superpixel stuff segmentation\n",
    "- 330K images\n",
    "- 1.5 million object instances\n",
    "- 80 object categories\n",
    "- 91 stuff categories\n",
    "- 5 captions per image\n",
    "- 250 000 people with keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the pre-trained YOLO model\n",
    "# You can use a more recent version of YOLO if there is one\n",
    "model = YOLO('yolo11n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b94fa-dcc4-4a0f-a1c4-93aaf9f6ff83",
   "metadata": {},
   "source": [
    "## Test the model on a scissors example\n",
    "\n",
    "The aim of this test is to show that the pre-trained model is currently unable to recognize the rock / paper / scissors game.\n",
    "\n",
    "You can consult the [Ultralytics Predict Documentation](https://docs.ultralytics.com/modes/predict/) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146f10e-7493-4d90-8113-f2ce0573fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('/workspace/data/test/images/20220216_221856_jpg.rf.c551cb3856f480cba36d6aa58e3300cd.jpg', project='/workspace/data/runs/predicts', verbose=True, save=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4fd08",
   "metadata": {},
   "source": [
    "By running this function, you will obtain an output in which the first line indicates the element(s) detected within the image. \n",
    "\n",
    "**Parameters definition**\n",
    "\n",
    "Within this function, we specify:\n",
    "- The `source` from which we want our prediction (can be an image path, video file, directory, URL, ...).\n",
    "- The minimum confidence threshold `conf`. Detections with confidence below this threshold will be disregarded.\n",
    "- A `save=True` parameter, which allow us to view the resulting image, which will be stored in the path indicated in the function output.\n",
    "\n",
    "FYI, you could also have run `model(image)`, which would also have given you the prediction. But the `predict()` function allows you to specify more parameters (save, conf, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c6463",
   "metadata": {},
   "source": [
    "## Run YOLO training on ü™® / üìÉ / ‚úÇÔ∏è dataset\n",
    "\n",
    "You can consult the [Ultralytics Train Documentation](https://docs.ultralytics.com/modes/train/) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf46a6-4cdb-4589-8e48-8655be005759",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.train(data='/workspace/data/data.yaml', device=0, epochs=5, project='/workspace/data/runs/trains', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d370357-b354-41a0-950d-558ccb234347",
   "metadata": {},
   "source": [
    "**Parameters definitions**:\n",
    "\n",
    "- `data`: refers to the path to the dataset `.yaml` file.\n",
    "- `device`: determines the device on which the model will be trained (we want to use GPU).\n",
    "- `epochs`: refers to the number of training epochs. An epoch corresponds to one training cycle through the full training dataset.\n",
    "- `project`: path where the information files are generated during the training\n",
    "- `verbose`: to display some additional execution logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33862e1a",
   "metadata": {},
   "source": [
    "## Display results of YOLO training on ü™® / üìÉ / ‚úÇÔ∏è dataset\n",
    "\n",
    "According to the last line of the training output, your results have been saved in the `/workspace/data/runs/trains/train` folder. This folder contains several images, showing the performance of the trained model.\n",
    "\n",
    "Let's display these images one by one and analyse them to understand how your trained model behaves !\n",
    "\n",
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display images\n",
    "from IPython.display import Image\n",
    "\n",
    "# display the confusion matrix image\n",
    "Image(filename='/workspace/data/runs/trains/train/confusion_matrix.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343197a",
   "metadata": {},
   "source": [
    "The matrix shows that the model correctly identified 132 instances of \"Paper\", 141 instances of \"Rock\", and 113 instances of \"Scissors\" (Main diagonal). It also shows that some instances were incorrectly classified.\n",
    "\n",
    "Overall, this matrix suggests that the model performs well in recognizing 'Rock', 'Paper', and 'Scissors' gestures, but there is still room for improvement with further training, given that the model has only been trained for 5 epochs.\n",
    "\n",
    "You can also vizualise the normalized confusion matrix file `confusion_matrix_normalized.png`, which represents the data in proportions rather than raw counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489b088-c4d0-41cd-be95-a0e8d61cebb8",
   "metadata": {},
   "source": [
    "### Groundtruth & Predicted Batch examples\n",
    "\n",
    "To better understand these numbers and see how well the model detects and classifies objects in images, we can vizualize and compare the ground truth labels for a set of images with the predictions made by the model for those same images. By visualizing the label and prediction images side by side, we can easily see how well the model is doing and where it might be making mistakes.\n",
    "\n",
    "These results are available for both the training set and the validation one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/val_batch2_labels.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cdda8-fb5c-4d6c-a7d6-0c5c3be5c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/val_batch2_pred.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2f6ac-4133-4814-a90b-4cdc8bbd7103",
   "metadata": {},
   "source": [
    "If we compare these two images, we can see that the model performs extremely well. In each image, the hand signs have been correctly classified and located (bounding boxes). Regarding the classifications, it made only one mistake, recognising a Scissors when there was none. However, the model was not that confident for this prediction, since it indicated a confidence index of 0.7. (lowest one among the visualized results), which can be filtered out later by setting a threshold (only keep the detections > 0,8 for example). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d009db7-0eb7-45d6-a6ae-22f57445f9b7",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b383a3-c7e4-4368-a0ff-7f9369a873b6",
   "metadata": {},
   "source": [
    "In the context of object detection & classification, **Precision**, **Recall**, and the **F1 score** are commonly used metrics to evaluate the performance of a model. Here's a brief explanation of each metric:\n",
    "\n",
    "- **Precision**: Precision is the measure of the number of correct predictions (true positive) made by the model out of detections made. In other words, it tells us what proportion of the objects detected by the model are actually present in the image.\n",
    "\n",
    "- **Recall**: Recall is the measure of the number of correct predictions made by the model out of all actual positive instances in the dataset (Groundtruth). In other words, it tells us what proportion of the objects present in the image are correctly detected by the model.\n",
    "\n",
    "- **F1 score**: The F1 score is the harmonic mean of precision and recall, which balances these 2 metrics to provide a single measure of the model's performance. F1 score makes it possible to find the optimum threshold value, with an interesting compromise between precision and recall.\n",
    "\n",
    "By computing these metrics, we can get a better understanding of how well the model is performing and identify areas where it might need improvement. A good model should have both high precision and recall, resulting in a high F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9860062-a7ef-46c1-8183-a01e1c5fd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/P_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6ed3c-a5b6-4dd7-a27f-622062ebc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/R_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bfe9d-2a60-4ffc-8857-c76af93ccfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/PR_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fd85f-ba8a-42a1-8d0f-1088d89cb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/F1_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fbf5b-21e6-4868-8d27-9d9923bb358c",
   "metadata": {},
   "source": [
    "### General result graphs\n",
    "This last image groups together several graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dba8ac-aecb-4e9c-8c61-10ea6a224c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/workspace/data/runs/trains/train/results.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba536f45-89f3-4178-8881-028002294f9f",
   "metadata": {},
   "source": [
    "### Graphs and functions explanation\n",
    "\n",
    "Three loss functions used to optimize the model's performance during training:\n",
    "\n",
    "- **box_loss**: This loss is used to optimize the model's ability to predict **accurate bounding boxes** around hand signs in an image. The box loss measures the difference between the predicted bounding box coordinates and the ground truth bounding box coordinates (this is what we call the *Intersection over Union* metric. As the metric decreases, the model becomes more accurate in its detection.\n",
    "- **cls_loss**: Classification Loss is used to optimize the model's ability to correctly classify hand signs in an image. It measures the difference between the predicted class and the ground truth class labels for each hand sign in an image. As this metric decreases, the model becomes more accurate in its classifications.\n",
    "- **dfl_loss**: Distribution Focal Loss is also used to optimize the bounding boxes, as the *box_loss*. But the key difference compared to this other metric is that the *dfl_loss* is designed to handle the class imbalance problem.\n",
    "\n",
    "By using these 3 loss functions, which quantify the difference between the predictions and the labels, both in terms of the location of the hand signs and their nature (Rock; Paper; Scissors), the model is able to improve itself between each epoch.\n",
    "\n",
    "**Evaluation Metric**:\n",
    "\n",
    "- **mAP (mean Average Precision)** Based on the previous metrics (Confusion Matrix, Precision, Recall & F1 Score, the mAP measures the quality of the model's predictions (location & class). Its value is between 0 and 1. The higher the score, the more accurate the model is in its detections. When using YOLO, the mAP is first computed for an IoU= 0.5: `mAP@ 0.5`, and then for different IoU thresholds, which gives `mAP@ 0.5:0.95` (from 0.5 to 0.95 in steps of 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63349687",
   "metadata": {},
   "source": [
    "## Export trained weights for future inference\n",
    "\n",
    "You can consult the [Ultralytics Export Documentation](https://docs.ultralytics.com/modes/export/) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the weights\n",
    "exportedWeights = model.export()\n",
    "print(exportedWeights)\n",
    "\n",
    "# Copy the final model to the /workspace/data/ folder\n",
    "import shutil\n",
    "shutil.copy(exportedWeights, '/workspace/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c2a41-fabc-49e5-b19a-21c981d50487",
   "metadata": {},
   "source": [
    "## Create a new YOLO model object based on the exported weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bf6b07-333f-4ad8-a8ef-8fdfc78eefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModel = YOLO(exportedWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7f109",
   "metadata": {},
   "source": [
    "## Test your YOLO custom model on the ü™® / üìÉ / ‚úÇÔ∏è dataset\n",
    "\n",
    "Perform inference on the contents of the `/workspace/dataset/test` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2babbd-0f4e-4981-b11c-4b05a7be266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model on an image from the test folder\n",
    "savedModel.predict('/workspace/data/test/images/20220216_221856_jpg.rf.c551cb3856f480cba36d6aa58e3300cd.jpg', project='/workspace/data/runs/predicts', verbose=True, save=True, conf=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7396e49-f410-4e99-9a91-bd605c341498",
   "metadata": {},
   "source": [
    "You can vizualize the resulting images: `/workspace/data/runs/predicts`\n",
    "\n",
    "‚úÖ The model recognised a rock with great confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
