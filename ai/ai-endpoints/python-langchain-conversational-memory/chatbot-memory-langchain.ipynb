{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec35f43a-a1b8-42bf-8e1e-004876c10ee9",
   "metadata": {},
   "source": [
    "# Tutorial - Chatbot memory management with LangChain and AI Endpoints\n",
    "\n",
    "*Use AI Endpoints and LangChain to implement **conversational memory** and enable your chatbot to better answer questions using its knowledge.*\n",
    "\n",
    "### Step 1 - Install Python dependencies\n",
    "\n",
    "> The following Python code is based on `Python3.11` version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98ae7d-262a-43f3-b27f-a5d8137a9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b6717-af88-4a9f-9973-4ff10d684ea5",
   "metadata": {},
   "source": [
    "### Step 2 - Create the .env file to store environment variables."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b66ba6c3-0fa0-4188-8ab7-37dd08879e91",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "LLM_AI_ENDPOINT=https://mistral-7b-instruct-v0-3.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\n",
    "OVH_AI_ENDPOINTS_ACCESS_TOKEN=<ai-endpoints-api-token>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce36d2-1de9-41c3-a83a-d6f880f1aff4",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è Test AI Endpoints and get your free token `ai-endpoints-api-token` [here](https://endpoints.ai.cloud.ovh.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076f1e9-bbf5-4120-963d-6ac5f73bdaa5",
   "metadata": {},
   "source": [
    "### Step 3 - Import Python librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d9291a-87b9-4f79-acb6-2a3aca4076b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d9b3c-e5fa-43a0-abde-09430c15d3d3",
   "metadata": {},
   "source": [
    "### Step 4 - Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db914a6-6bf1-4d45-a9bb-7dd20eaee358",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# access the environment variables from the .env file\n",
    "ai_endpoint_token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n",
    "ai_endpoint_mistral7b = os.getenv(\"LLM_AI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019724a6-101e-4ac9-807e-923b129eccef",
   "metadata": {},
   "source": [
    "### Step 5 - Test Mistral7b without Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfe8547-9854-499d-ac19-b1db599560fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§: Hello, my name is Elea\n",
      "ü§ñ: Hello Elea, nice to meet you. How can I assist you today?\n",
      "üë§: What is the capital of France?\n",
      "ü§ñ: The capital city of France is Paris. Paris is one of the most famous and visited cities in the world. It is known for its art, culture, and cuisine.\n",
      "üë§: Do you know my name?\n",
      "ü§ñ: I'm an assistant and I don't have the ability to know your name without being told.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(\n",
    "        model_name=\"Mistral-7B-Instruct-v0.3\", \n",
    "        openai_api_key=ai_endpoint_token,\n",
    "        openai_api_base=ai_endpoint_mistral7b, \n",
    "        max_tokens=512,\n",
    "        temperature=0.0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are an assistant. Answer to the question.\"),\n",
    "(\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Create the conversation chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Start the conversation\n",
    "question = \"Hello, my name is Elea\"\n",
    "response = chain.invoke(question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response.content}\")\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "response = chain.invoke(question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response.content}\")\n",
    "\n",
    "question = \"Do you know my name?\"\n",
    "response = chain.invoke(question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b627cb1-a510-4a73-a244-f893c3b3ccda",
   "metadata": {},
   "source": [
    "### Step 6 - Add Memory Window to your LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a887b58-f721-493f-8ec0-4ee56e53adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§: Hello, my name is Elea\n",
      "ü§ñ: Hello Elea, nice to meet you. I'm an AI designed to assist and engage in friendly conversations. How can I help you today? Would you like to know a joke, play a game, or discuss a specific topic? I'm here to help and provide lots of specific details from my context. If I don't know the answer to a question, I'll truthfully say I don't know. So, what would you like to talk about today? I'm all ears!\n",
      "üë§: What is the capital of France?\n",
      "ü§ñ: The capital city of France is Paris. Paris is one of the most famous and romantic cities in the world. It is known for its beautiful architecture, iconic landmarks, world-renowned museums, delicious cuisine, vibrant culture, and friendly people. Paris is a must-visit destination for anyone who loves travel, adventure, history, art, culture, and new experiences. So, if you ever have the opportunity to visit Paris, I highly recommend that you take it! You won't be disappointed!\n",
      "üë§: Do you know my name?\n",
      "ü§ñ: Yes, I do. Your name is Elea. How can I help you today, Elea? Would you like to know a joke, play a game, or discuss a specific topic? I'm here to help and provide lots of specific details from my context. If I don't know the answer to a question, I'll truthfully say I don't know. So, what would you like to talk about today, Elea? I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(\n",
    "        model_name=\"Mistral-7B-Instruct-v0.3\", \n",
    "        openai_api_key=ai_endpoint_token,\n",
    "        openai_api_base=ai_endpoint_mistral7b,\n",
    "        max_tokens=512,\n",
    "        temperature=0.0\n",
    ")\n",
    "\n",
    "# Add Converstion Window Memory\n",
    "memory = ConversationBufferWindowMemory(k=10)\n",
    "\n",
    "# Create the conversation chain\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Start the conversation\n",
    "question = \"Hello, my name is Elea\"\n",
    "response = conversation.predict(input=question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response}\")\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "response = conversation.predict(input=question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response}\")\n",
    "\n",
    "question = \"Do you know my name?\"\n",
    "response = conversation.predict(input=question)\n",
    "print(f\"üë§: {question}\")\n",
    "print(f\"ü§ñ: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
